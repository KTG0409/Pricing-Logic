#!/usr/bin/env python3
"""
Zone Optimization Script v7 - Complete Implementation with Historical Analysis
Integrates zone optimization analysis into your existing category analysis pipeline.

Enhanced for 6-week accelerated timeline using 2+ years of historical data.
Designed specifically for customer-week level data aggregation.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
from collections import defaultdict, deque
import json
import warnings
from scipy import stats
import os
warnings.filterwarnings('ignore')

class ZoneOptimizationEngine:
    """
    Learns optimal zone suffixes for each Company + NPD Cuisine + Attribute Group combination
    to maximize volume and customer count growth, accepting margin sacrifice when overpriced.
    Enhanced for historical analysis with 2+ years of data for accelerated implementation.
    """
    
    def __init__(self, learning_window_weeks=26, min_sample_size=3, historical_analysis_mode=True):
        self.learning_window_weeks = learning_window_weeks
        self.min_sample_size = min_sample_size
        self.historical_analysis_mode = historical_analysis_mode  # NEW: Enable historical analysis
        
        # Core learning structures for zone performance
        self.zone_performance = defaultdict(lambda: {
            'volume_elasticity': 0.0,
            'customer_elasticity': 0.0,
            'optimal_zone_estimate': 5.0,
            'zone_effectiveness_curve': {},
            'sample_count': 0,
            'confidence': 0.0,
            'last_updated': datetime.now(),
            'historical_performance': []
        })
        
        # NEW: Historical pattern storage
        self.historical_patterns = defaultdict(list)
        self.proven_zone_changes = {}
        self.failed_zone_changes = {}
        self.high_confidence_recommendations = []
        
        # Track zone change outcomes
        self.zone_change_outcomes = deque(maxlen=500)
        
        # Granularity expansion tracking
        self.granularity_needs = defaultdict(lambda: {
            'needs_expansion': False,
            'recommended_zones_between_0_1': 0,
            'zero_recommendations_count': 0,
            'evidence_score': 0.0
        })
        
        # Price sensitivity learning
        self.price_sensitivity_patterns = defaultdict(list)
    
    def analyze_historical_zone_changes(self, full_historical_df: pd.DataFrame) -> Dict:
        """
        NEW: Analyze 2+ years of historical data to identify proven zone change patterns
        """
        
        print("Analyzing historical zone change patterns...")
        
        # Sort by date to track zone changes over time
        historical_df = full_historical_df.copy()
        
        # Extract zone features for all historical data
        historical_df = self.extract_zone_features(historical_df)
        
        # Group by Company_Combo_Key and track zone changes over time
        historical_patterns = {}
        
        for company_combo in historical_df['Company_Combo_Key'].unique():
            combo_data = historical_df[historical_df['Company_Combo_Key'] == company_combo].copy()
            
            if len(combo_data) < 4:  # Need substantial historical data
                continue
            
            # Sort by time period (assuming you have date/period columns)
            if 'Fiscal Week Number' in combo_data.columns:
                combo_data = combo_data.sort_values(['Fiscal Week Number'])
            
            # Analyze zone effectiveness over time
            zone_performance_over_time = []
            
            for zone in combo_data['Zone_Suffix_Numeric'].unique():
                zone_data = combo_data[combo_data['Zone_Suffix_Numeric'] == zone]
                
                if len(zone_data) >= 3:  # Need multiple data points per zone
                    avg_performance = zone_data['Combined_Performance_Score'].mean()
                    total_volume = zone_data['Pounds_CY'].sum()
                    weeks_used = len(zone_data)
                    
                    zone_performance_over_time.append({
                        'zone': zone,
                        'avg_performance': avg_performance,
                        'total_volume': total_volume,
                        'weeks_tested': weeks_used,
                        'volume_growth_rate': zone_data['Volume_Growth_Rate'].mean()
                    })
            
            if zone_performance_over_time:
                # Sort by performance to find historical optimal zones
                zone_performance_over_time.sort(key=lambda x: x['avg_performance'], reverse=True)
                
                historical_patterns[company_combo] = {
                    'company_name': combo_data['Company Name'].iloc[0],
                    'combo_key': combo_data['Combo_Key'].iloc[0],
                    'total_historical_volume': combo_data['Pounds_CY'].sum(),
                    'zones_tested': [zp['zone'] for zp in zone_performance_over_time],
                    'historical_best_zone': zone_performance_over_time[0]['zone'],
                    'historical_best_performance': zone_performance_over_time[0]['avg_performance'],
                    'zone_performance_history': zone_performance_over_time,
                    'confidence': self._calculate_historical_confidence(zone_performance_over_time),
                    'proven_pattern': self._identify_proven_pattern(zone_performance_over_time)
                }
        
        self.historical_patterns = historical_patterns
        return historical_patterns
    
    def _calculate_historical_confidence(self, zone_performance_history: List[Dict]) -> float:
        """Calculate confidence based on historical data depth and consistency"""
        
        if not zone_performance_history:
            return 0.0
        
        # More zones tested = higher confidence
        zones_tested = len(zone_performance_history)
        zone_confidence = min(zones_tested / 5.0, 0.4)  # Max 40% from variety
        
        # More data points = higher confidence  
        total_weeks = sum(zp['weeks_tested'] for zp in zone_performance_history)
        time_confidence = min(total_weeks / 50.0, 0.4)  # Max 40% from sample size
        
        # Performance separation = higher confidence
        if len(zone_performance_history) >= 2:
            best_performance = zone_performance_history[0]['avg_performance']
            second_best = zone_performance_history[1]['avg_performance']
            separation = abs(best_performance - second_best)
            separation_confidence = min(separation / 0.3, 0.2)  # Max 20% from clear winner
        else:
            separation_confidence = 0.0
        
        return min(zone_confidence + time_confidence + separation_confidence, 0.95)
    
    def _identify_proven_pattern(self, zone_performance_history: List[Dict]) -> str:
        """Identify the type of proven pattern from historical data"""
        
        if not zone_performance_history:
            return "INSUFFICIENT_DATA"
        
        zones_by_performance = [zp['zone'] for zp in zone_performance_history]
        best_zone = zones_by_performance[0]
        
        # Check for Zone 0 tendency
        if best_zone <= 1:
            return "GRANULARITY_EXPANSION_VALIDATED"
        
        # Check for clear zone preference
        if len(zone_performance_history) >= 3:
            best_performance = zone_performance_history[0]['avg_performance']
            avg_others = np.mean([zp['avg_performance'] for zp in zone_performance_history[1:]])
            
            if best_performance > avg_others + 0.2:
                return "CLEAR_OPTIMAL_ZONE"
            else:
                return "MARGINAL_DIFFERENCES"
        
        return "LIMITED_COMPARISON"
    
    def generate_immediate_high_confidence_recommendations(self, current_data_df: pd.DataFrame) -> List[Dict]:
        """
        NEW: Generate immediate recommendations based on historical patterns
        """
        
        if not self.historical_patterns:
            return self.generate_zone_recommendations({})  # Fall back to standard analysis
        
        print("Generating high-confidence historical recommendations...")
        
        # Extract current zone features
        current_df = self.extract_zone_features(current_data_df)
        
        immediate_recommendations = []
        
        for company_combo, historical_analysis in self.historical_patterns.items():
            
            # Check if this combo exists in current data
            current_combo_data = current_df[current_df['Company_Combo_Key'] == company_combo]
            
            if current_combo_data.empty:
                continue
            
            current_zone = current_combo_data['Zone_Suffix_Numeric'].iloc[0]
            historical_best_zone = historical_analysis['historical_best_zone']
            confidence = historical_analysis['confidence']
            
            # Only recommend if high confidence and meaningful change needed
            if confidence >= 0.7 and abs(current_zone - historical_best_zone) >= 1:
                
                # Calculate expected impact based on historical data
                historical_best_performance = historical_analysis['historical_best_performance']
                current_performance_estimate = current_combo_data['Combined_Performance_Score'].iloc[0]
                
                expected_improvement = historical_best_performance - current_performance_estimate
                current_volume = current_combo_data['Pounds_CY'].sum()
                
                rec = {
                    'type': 'HISTORICAL_PROVEN',
                    'company_combo': company_combo,
                    'company_name': historical_analysis['company_name'],
                    'combo_description': historical_analysis['combo_key'],
                    'current_zone': current_zone,
                    'recommended_zone': historical_best_zone,
                    'confidence': confidence,
                    'historical_evidence': f"Best zone historically with {historical_analysis['confidence']:.0%} confidence",
                    'expected_volume_impact': self._estimate_historical_volume_impact(expected_improvement, current_volume),
                    'total_volume_at_stake': current_volume,
                    'implementation_priority': self._calculate_historical_priority(confidence, current_volume, expected_improvement),
                    'expected_timeline': "2-3 weeks (historically proven pattern)",
                    'risk_assessment': self._assess_historical_risk(historical_analysis, current_zone, historical_best_zone),
                    'proven_pattern': historical_analysis['proven_pattern'],
                    'historical_zones_tested': historical_analysis['zones_tested'],
                    'specific_actions': [f"Change from Zone {current_zone} to Zone {historical_best_zone}"]
                }
                
                immediate_recommendations.append(rec)
        
        # Sort by implementation priority
        immediate_recommendations.sort(key=lambda x: x['implementation_priority'], reverse=True)
        
        print(f"Generated {len(immediate_recommendations)} high-confidence historical recommendations")
        
        return immediate_recommendations
    
    def _estimate_historical_volume_impact(self, performance_improvement: float, current_volume: float) -> str:
        """Estimate volume impact based on historical performance improvement"""
        
        if performance_improvement > 0.3:
            return f"20-30% volume increase (${current_volume * 0.25:,.0f})"
        elif performance_improvement > 0.2:
            return f"15-20% volume increase (${current_volume * 0.175:,.0f})"
        elif performance_improvement > 0.1:
            return f"10-15% volume increase (${current_volume * 0.125:,.0f})"
        elif performance_improvement > 0:
            return f"5-10% volume increase (${current_volume * 0.075:,.0f})"
        else:
            return "Minimal impact expected"
    
    def _calculate_historical_priority(self, confidence: float, volume: float, improvement: float) -> float:
        """Calculate priority for historical recommendations"""
        
        priority = 0.0
        
        # Confidence weight (0-40 points)
        priority += confidence * 40
        
        # Volume weight (0-30 points)  
        priority += min(volume / 10000, 30)
        
        # Improvement potential (0-30 points)
        priority += min(improvement * 100, 30)
        
        return min(priority, 100)
    
    def _assess_historical_risk(self, historical_analysis: Dict, current_zone: float, recommended_zone: float) -> Dict:
        """Assess risk based on historical patterns"""
        
        risk_factors = []
        risk_level = "LOW"
        
        # Large zone change risk
        zone_change = abs(current_zone - recommended_zone)
        if zone_change > 2:
            risk_factors.append(f"Large zone change: {current_zone} to {recommended_zone}")
            risk_level = "MEDIUM"
        
        # High volume risk
        if historical_analysis['total_historical_volume'] > 50000:
            risk_factors.append("High historical volume - significant business impact")
            risk_level = "MEDIUM"
        
        # Pattern reliability
        if historical_analysis['proven_pattern'] == "MARGINAL_DIFFERENCES":
            risk_factors.append("Historically marginal differences between zones")
            if risk_level == "LOW":
                risk_level = "MEDIUM"
        
        # Confidence-based risk adjustment
        if historical_analysis['confidence'] > 0.9:
            risk_level = "LOW"  # Override to low risk for very high confidence
        
        return {
            'risk_level': risk_level,
            'risk_factors': risk_factors,
            'mitigation_strategies': self._historical_risk_mitigation(risk_factors, historical_analysis)
        }
    
    def _historical_risk_mitigation(self, risk_factors: List[str], historical_analysis: Dict) -> List[str]:
        """Suggest risk mitigation for historical recommendations"""
        
        mitigations = []
        
        if any("Large zone change" in factor for factor in risk_factors):
            mitigations.append("Phase implementation: test intermediate zone first for 1 week")
        
        if any("High historical volume" in factor for factor in risk_factors):
            mitigations.append("Monitor daily for first 5 days, weekly thereafter")
        
        if any("marginal differences" in factor for factor in risk_factors):
            mitigations.append("Set clear success metrics and 2-week evaluation checkpoint")
        
        # Always include historical context
        zones_tested = ", ".join(map(str, historical_analysis['zones_tested']))
        mitigations.append(f"Historical context: Previously tested zones {zones_tested}")
        
        return mitigations
    
    def extract_zone_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Extract zone optimization features from data"""
        
        zone_df = df.copy()
        
        # Parse Price Zone ID to get Zone Suffix
        if 'Price Zone ID' in zone_df.columns:
            zone_df['Zone_Suffix'] = zone_df['Price Zone ID'].astype(str).str.split('-').str[-1]
            zone_df['Zone_Suffix_Numeric'] = pd.to_numeric(zone_df['Zone_Suffix'], errors='coerce').fillna(999)
        else:
            # If no Price Zone ID, try to infer from other columns or create dummy
            zone_df['Zone_Suffix'] = '5'  # Default middle zone
            zone_df['Zone_Suffix_Numeric'] = 5
        
        # Create the significant combo key
        zone_df['Combo_Key'] = (
            zone_df.get('NPD Cuisine Type', 'Unknown').astype(str) + '_' +
            zone_df.get('Attribute Group ID', 'Unknown').astype(str) + '_' +
            zone_df.get('Price Source Type', 'Unknown').astype(str)
        )
        
        # Company-specific combo key
        zone_df['Company_Combo_Key'] = (
            zone_df.get('Company Name', 'Unknown').astype(str) + '_' + zone_df['Combo_Key']
        )
        
        # Performance metrics
        zone_df['Volume_Performance'] = zone_df.get('Delta_YoY_Lbs', 0)
        zone_df['Volume_Growth_Rate'] = np.where(
            zone_df.get('Pounds_PY', 0) > 0,
            zone_df.get('Delta_YoY_Lbs', 0) / zone_df.get('Pounds_PY', 1),
            0
        )
        
        # Customer metrics (if available)
        if 'Distinct_Customers_CY' in zone_df.columns and 'Distinct_Customers_PY' in zone_df.columns:
            zone_df['Customer_Growth'] = zone_df['Distinct_Customers_CY'] - zone_df['Distinct_Customers_PY']
            zone_df['Customer_Growth_Rate'] = np.where(
                zone_df['Distinct_Customers_PY'] > 0,
                zone_df['Customer_Growth'] / zone_df['Distinct_Customers_PY'],
                0
            )
        else:
            # Estimate distinct customers from volume patterns
            zone_df['Customer_Growth'] = zone_df['Volume_Performance'] * 0.1  # Rough proxy
            zone_df['Customer_Growth_Rate'] = zone_df['Volume_Growth_Rate'] * 0.8  # Customers less elastic
        
        # Combined performance score (weighted toward volume and customers)
        zone_df['Combined_Performance_Score'] = (
            zone_df['Volume_Growth_Rate'] * 0.6 +  # Volume gets 60% weight
            zone_df['Customer_Growth_Rate'] * 0.4   # Customers get 40% weight
        )
        
        return zone_df
    
    def analyze_zone_effectiveness(self, zone_df: pd.DataFrame) -> Dict:
        """Analyze how different zones perform for each company-combo"""
        
        zone_analysis = {}
        
        for company_combo in zone_df['Company_Combo_Key'].unique():
            
            combo_data = zone_df[zone_df['Company_Combo_Key'] == company_combo].copy()
            
            if len(combo_data) < 2:  # Need multiple data points
                continue
            
            # Group by zone suffix to see performance patterns
            zone_performance = combo_data.groupby('Zone_Suffix_Numeric').agg({
                'Volume_Performance': 'sum',
                'Volume_Growth_Rate': 'mean',
                'Customer_Growth': 'sum', 
                'Customer_Growth_Rate': 'mean',
                'Combined_Performance_Score': 'mean',
                'Pounds_CY': 'sum',
                'Company Name': 'first',
                'Combo_Key': 'first'
            }).reset_index()
            
            # Sort by zone to see the pricing curve
            zone_performance = zone_performance.sort_values('Zone_Suffix_Numeric')
            
            # Find optimal zone (best combined performance)
            if not zone_performance.empty:
                best_zone_idx = zone_performance['Combined_Performance_Score'].idxmax()
                optimal_zone = zone_performance.loc[best_zone_idx, 'Zone_Suffix_Numeric']
                best_performance = zone_performance.loc[best_zone_idx, 'Combined_Performance_Score']
                
                # Calculate price elasticity (how performance changes with zone)
                if len(zone_performance) >= 3:
                    zones = zone_performance['Zone_Suffix_Numeric'].values
                    scores = zone_performance['Combined_Performance_Score'].values
                    
                    # Simple elasticity: change in performance per zone change
                    zone_changes = np.diff(zones)
                    score_changes = np.diff(scores)
                    elasticities = np.where(zone_changes != 0, score_changes / zone_changes, 0)
                    avg_elasticity = np.mean(elasticities)
                else:
                    avg_elasticity = 0
                
                zone_analysis[company_combo] = {
                    'company_name': combo_data['Company Name'].iloc[0],
                    'combo_key': combo_data['Combo_Key'].iloc[0],
                    'current_zones_tested': zone_performance['Zone_Suffix_Numeric'].tolist(),
                    'zone_performance_curve': zone_performance.to_dict('records'),
                    'optimal_zone': optimal_zone,
                    'optimal_performance': best_performance,
                    'price_elasticity': avg_elasticity,
                    'total_volume': combo_data['Pounds_CY'].sum(),
                    'needs_analysis': self._assess_analysis_needs(zone_performance, optimal_zone)
                }
        
        return zone_analysis
    
    def _assess_analysis_needs(self, zone_performance: pd.DataFrame, optimal_zone: float) -> Dict:
        """Assess what kind of analysis or action is needed for this combo"""
        
        needs = {
            'type': 'STABLE',
            'recommendations': [],
            'confidence': 'HIGH',
            'priority': 'LOW'
        }
        
        zones_tested = set(zone_performance['Zone_Suffix_Numeric'].tolist())
        performance_scores = zone_performance['Combined_Performance_Score'].tolist()
        
        # Check if optimal zone is at boundary (need to test more zones)
        min_zone_tested = min(zones_tested)
        max_zone_tested = max(zones_tested)
        
        if optimal_zone == min_zone_tested and min_zone_tested > 0:
            needs['type'] = 'TEST_LOWER_ZONES'
            needs['recommendations'].append(f"Test zones {max(0, min_zone_tested-2)}-{min_zone_tested-1}")
            needs['confidence'] = 'MEDIUM'
            needs['priority'] = 'MEDIUM'
            
            # Special case: if optimal is zone 1 or 2, flag for granularity expansion
            if optimal_zone <= 2:
                needs['granularity_expansion_candidate'] = True
                needs['recommendations'].append("Consider adding zones 0.1, 0.2, 0.5 for finer pricing")
        
        elif optimal_zone == max_zone_tested:
            needs['type'] = 'TEST_HIGHER_ZONES' 
            needs['recommendations'].append(f"Test zones {max_zone_tested+1}-{max_zone_tested+3}")
            needs['confidence'] = 'MEDIUM'
            needs['priority'] = 'LOW'  # Less urgent since we're not at the cheap end
        
        # Check if we're recommending zone 0 (which we won't implement)
        if optimal_zone == 0:
            needs['type'] = 'GRANULARITY_EXPANSION_NEEDED'
            needs['recommendations'] = [
                "ZONE 0 RECOMMENDED - DO NOT IMPLEMENT",
                "Create zones 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 between current 0 and 1",
                "This combo shows high price sensitivity - needs finer pricing granularity"
            ]
            needs['confidence'] = 'HIGH'
            needs['priority'] = 'HIGH'
        
        # Check for strong price sensitivity (big performance differences between zones)
        if len(performance_scores) >= 3:
            performance_range = max(performance_scores) - min(performance_scores)
            if performance_range > 0.3:  # Large performance swing across zones
                needs['high_price_sensitivity'] = True
                if needs['type'] == 'STABLE':
                    needs['type'] = 'HIGH_SENSITIVITY'
                    needs['recommendations'].append("Monitor closely - high price sensitivity detected")
                    needs['priority'] = 'MEDIUM'
        
        return needs
    
    def generate_zone_recommendations(self, zone_analysis: Dict) -> List[Dict]:
        """Generate specific zone change recommendations"""
        
        recommendations = []
        
        for company_combo, analysis in zone_analysis.items():
            
            needs = analysis['needs_analysis']
            current_optimal = analysis['optimal_zone']
            total_volume = analysis['total_volume']
            
            # Skip low-volume combos unless they show exceptional promise
            if total_volume < 1000 and analysis['optimal_performance'] < 0.2:
                continue
            
            rec = {
                'company_combo': company_combo,
                'company_name': analysis['company_name'],
                'combo_description': analysis['combo_key'],
                'current_optimal_zone': current_optimal,
                'current_performance': analysis['optimal_performance'],
                'total_volume_at_stake': total_volume,
                'recommendation_type': needs['type'],
                'specific_actions': needs['recommendations'],
                'implementation_priority': self._calculate_zone_priority(analysis, needs),
                'expected_timeline': self._estimate_implementation_timeline(needs),
                'risk_assessment': self._assess_zone_change_risk(analysis, needs)
            }
            
            # Add specific zone targets for actionable recommendations
            if needs['type'] == 'TEST_LOWER_ZONES':
                suggested_zones = self._suggest_specific_lower_zones(analysis)
                rec['suggested_test_zones'] = suggested_zones
                rec['expected_volume_impact'] = self._estimate_volume_impact(analysis, suggested_zones)
            
            elif needs['type'] == 'GRANULARITY_EXPANSION_NEEDED':
                rec['granularity_expansion_plan'] = self._create_granularity_plan(analysis)
                rec['expected_volume_impact'] = "15-30% increase with proper granularity"
            
            recommendations.append(rec)
        
        return sorted(recommendations, key=lambda x: x['implementation_priority'], reverse=True)
    
    def _calculate_zone_priority(self, analysis: Dict, needs: Dict) -> float:
        """Calculate implementation priority (0-100)"""
        
        priority = 0.0
        
        # Volume impact potential
        volume_factor = min(analysis['total_volume'] / 10000, 30)  # Max 30 points
        priority += volume_factor
        
        # Performance opportunity
        if analysis['optimal_performance'] > 0.1:
            priority += 25  # Good performance opportunity
        elif analysis['optimal_performance'] > 0.05:
            priority += 15
        
        # Urgency based on need type
        if needs['type'] == 'GRANULARITY_EXPANSION_NEEDED':
            priority += 35  # High urgency - zone 0 recommendations
        elif needs['type'] == 'TEST_LOWER_ZONES':
            priority += 20  # Medium urgency - potential for lower pricing
        elif needs['type'] == 'HIGH_SENSITIVITY':
            priority += 10  # Monitor closely
        
        # Price elasticity bonus (more responsive = higher priority)
        if abs(analysis['price_elasticity']) > 0.1:
            priority += 10
        
        return min(priority, 100)
    
    def _suggest_specific_lower_zones(self, analysis: Dict) -> List[int]:
        """Suggest specific lower zones to test"""
        
        current_optimal = analysis['optimal_zone']
        zones_tested = set(analysis['current_zones_tested'])
        
        suggestions = []
        
        # Test 2-3 zones below current optimal
        for test_zone in range(max(0, int(current_optimal) - 3), int(current_optimal)):
            if test_zone not in zones_tested and test_zone >= 0:
                suggestions.append(test_zone)
        
        return suggestions[:3]  # Limit to 3 suggestions
    
    def _create_granularity_plan(self, analysis: Dict) -> Dict:
        """Create a plan for adding granular zones between 0 and 1"""
        
        return {
            'problem': 'Optimal zone recommendation is 0, but we will not implement zone 0',
            'solution': 'Add granular zones between 0 and 1',
            'suggested_new_zones': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
            'implementation_approach': 'Start with 0.5 and 0.8, then add others based on results',
            'expected_outcome': 'Find sweet spot that maximizes volume without going to zone 0',
            'monitoring_plan': 'Test 0.5 for 4 weeks, then 0.8 for 4 weeks, compare to current zone 1'
        }
    
    def _estimate_volume_impact(self, analysis: Dict, suggested_zones: List[int]) -> str:
        """Estimate volume impact from testing suggested zones"""
        
        elasticity = analysis['price_elasticity']
        current_optimal = analysis['optimal_zone']
        
        if not suggested_zones:
            return "Unable to estimate - no valid test zones"
        
        # Estimate impact of moving to lowest suggested zone
        lowest_zone = min(suggested_zones)
        zone_change = current_optimal - lowest_zone
        
        if abs(elasticity) > 0.05:  # Meaningful elasticity
            estimated_impact = zone_change * elasticity * 100
            if estimated_impact > 0:
                return f"Potential {estimated_impact:.0f}% volume increase"
            else:
                return f"Risk of {abs(estimated_impact):.0f}% volume decrease"
        else:
            return "Low price sensitivity - modest impact expected"
    
    def _estimate_implementation_timeline(self, needs: Dict) -> str:
        """Estimate timeline for implementing recommendations"""
        
        if needs['type'] == 'GRANULARITY_EXPANSION_NEEDED':
            return "8-12 weeks (requires zone structure changes)"
        elif needs['type'] in ['TEST_LOWER_ZONES', 'TEST_HIGHER_ZONES']:
            return "2-4 weeks (simple zone adjustments)"
        elif needs['type'] == 'HIGH_SENSITIVITY':
            return "Ongoing monitoring (no immediate action)"
        else:
            return "4-6 weeks"
    
    def _assess_zone_change_risk(self, analysis: Dict, needs: Dict) -> Dict:
        """Assess risk of implementing zone changes"""
        
        risk_factors = []
        risk_level = "LOW"
        
        volume = analysis['total_volume']
        elasticity = abs(analysis['price_elasticity'])
        
        # High volume at stake
        if volume > 10000:
            risk_factors.append(f"High volume at stake: ${volume:,.0f}")
            risk_level = "MEDIUM"
        
        # High price sensitivity
        if elasticity > 0.2:
            risk_factors.append("High price sensitivity - changes will have major impact")
            risk_level = "MEDIUM"
        
        # Granularity expansion risks
        if needs['type'] == 'GRANULARITY_EXPANSION_NEEDED':
            risk_factors.append("Requires significant zone structure changes")
            risk_level = "HIGH"
        
        # Limited testing data
        zones_tested = len(analysis['current_zones_tested'])
        if zones_tested < 3:
            risk_factors.append(f"Limited zone testing data ({zones_tested} zones)")
            if risk_level == "LOW":
                risk_level = "MEDIUM"
        
        return {
            'risk_level': risk_level,
            'risk_factors': risk_factors,
            'mitigation_strategies': self._suggest_risk_mitigation(risk_factors, needs)
        }
    
    def _suggest_risk_mitigation(self, risk_factors: List[str], needs: Dict) -> List[str]:
        """Suggest risk mitigation strategies"""
        
        mitigations = []
        
        if any("High volume" in factor for factor in risk_factors):
            mitigations.append("Phase implementation: start with 25% of volume for 2 weeks")
        
        if any("High price sensitivity" in factor for factor in risk_factors):
            mitigations.append("Monitor daily for first week, weekly thereafter")
        
        if any("zone structure changes" in factor for factor in risk_factors):
            mitigations.append("Coordinate with pricing team, IT, and operations before implementation")
        
        if any("Limited zone testing" in factor for factor in risk_factors):
            mitigations.append("Start with most conservative zone recommendation first")
        
        if needs['type'] == 'GRANULARITY_EXPANSION_NEEDED':
            mitigations.append("Test zone 0.8 first as safest starting point near zone 1")
        
        return mitigations
    
    def predict_volume_turnaround_timeline(self, recommendations: List[Dict], 
                                         current_trend: Dict) -> Dict:
        """Predict when volume will turn positive based on zone optimization recommendations"""
        
        # Current situation
        current_decline_rate = current_trend.get('volume_decline_rate_pct', 0)
        total_volume = current_trend.get('total_volume', 0)
        
        if total_volume == 0:
            return {'error': 'No volume data for prediction'}
        
        # Analyze recommendations by implementation timeline
        immediate_impact_recs = [r for r in recommendations if '2-4 weeks' in r.get('expected_timeline', '')]
        medium_term_recs = [r for r in recommendations if '4-6 weeks' in r.get('expected_timeline', '')]
        long_term_recs = [r for r in recommendations if '8-12 weeks' in r.get('expected_timeline', '')]
        
        # Estimate volume impact from each wave
        immediate_volume_lift = sum(self._extract_volume_impact(r) for r in immediate_impact_recs)
        medium_volume_lift = sum(self._extract_volume_impact(r) for r in medium_term_recs)
        long_volume_lift = sum(self._extract_volume_impact(r) for r in long_term_recs)
        
        total_potential_lift = immediate_volume_lift + medium_volume_lift + long_volume_lift
        
        # Calculate turnaround timeline
        if total_potential_lift > abs(current_decline_rate * total_volume):
            # We can overcome the decline
            if immediate_volume_lift > abs(current_decline_rate * total_volume * 0.7):
                turnaround_week = 6  # Fast turnaround
            elif immediate_volume_lift + medium_volume_lift > abs(current_decline_rate * total_volume):
                turnaround_week = 10  # Medium turnaround
            else:
                turnaround_week = 16  # Longer turnaround
        else:
            turnaround_week = None  # Need more aggressive action
        
        return {
            'current_decline_rate_pct': current_decline_rate,
            'total_volume_base': total_volume,
            'potential_volume_lift': total_potential_lift,
            'immediate_impact_volume': immediate_volume_lift,
            'medium_term_impact_volume': medium_volume_lift,
            'long_term_impact_volume': long_volume_lift,
            'predicted_turnaround_week': turnaround_week,
            'confidence': self._calculate_prediction_confidence(recommendations, total_potential_lift),
            'key_dependencies': [
                f"{len(immediate_impact_recs)} immediate-impact zone changes",
                f"{len(long_term_recs)} granularity expansions", 
                "Successful implementation without major operational issues"
            ]
        }
    
    def _extract_volume_impact(self, recommendation: Dict) -> float:
        """Extract estimated volume impact from a recommendation"""
        
        volume_at_stake = recommendation.get('total_volume_at_stake', 0)
        
        # Parse expected impact
        impact_text = recommendation.get('expected_volume_impact', '0')
        if 'increase' in str(impact_text).lower():
            # Extract percentage
            import re
            pct_match = re.search(r'(\d+)%', str(impact_text))
            if pct_match:
                pct_increase = float(pct_match.group(1)) / 100
                return volume_at_stake * pct_increase
        
        # Default conservative estimate based on recommendation type
        if recommendation['recommendation_type'] == 'GRANULARITY_EXPANSION_NEEDED':
            return volume_at_stake * 0.2  # 20% conservative estimate
        elif recommendation['recommendation_type'] == 'TEST_LOWER_ZONES':
            return volume_at_stake * 0.1  # 10% conservative estimate
        
        return 0
    
    def _calculate_prediction_confidence(self, recommendations: List[Dict], 
                                       total_lift: float) -> float:
        """Calculate confidence in the turnaround prediction"""
        
        confidence = 0.5  # Start at 50%
        
        # More recommendations = higher confidence (up to a point)
        rec_confidence = min(len(recommendations) / 10.0, 0.3)
        confidence += rec_confidence
        
        # Higher volume impact = higher confidence 
        if total_lift > 0:
            impact_confidence = min(total_lift / 50000, 0.2)  # Cap at 20%
            confidence += impact_confidence
        
        return min(confidence, 0.9)  # Cap at 90%
    
    def generate_executive_dashboard(self, recommendations: List[Dict], 
                                   turnaround_prediction: Dict) -> Dict:
        """Generate executive dashboard for zone optimization"""
        
        # Summary statistics
        high_priority = [r for r in recommendations if r['implementation_priority'] > 80]
        granularity_expansions = [r for r in recommendations if r['recommendation_type'] == 'GRANULARITY_EXPANSION_NEEDED']
        zone_tests = [r for r in recommendations if 'TEST' in r['recommendation_type']]
        
        total_volume_opportunity = sum(r['total_volume_at_stake'] for r in recommendations)
        
        return {
            'executive_summary': {
                'total_zone_opportunities': len(recommendations),
                'high_priority_actions': len(high_priority),
                'granularity_expansions_needed': len(granularity_expansions),
                'zone_tests_recommended': len(zone_tests),
                'total_volume_in_scope': total_volume_opportunity,
                'predicted_turnaround_week': turnaround_prediction.get('predicted_turnaround_week', 'TBD')
            },
            'immediate_action_items': [
                {
                    'company': r['company_name'],
                    'combo': r['combo_description'],
                    'action': r['specific_actions'][0] if r['specific_actions'] else 'Review needed',
                    'volume': r['total_volume_at_stake'],
                    'priority': r['implementation_priority']
                }
                for r in high_priority[:5]  # Top 5 actions
            ],
            'granularity_expansion_alerts': [
                {
                    'company': r['company_name'], 
                    'combo': r['combo_description'],
                    'issue': 'ZONE 0 RECOMMENDED - CREATE FRACTIONAL ZONES',
                    'volume': r['total_volume_at_stake']
                }
                for r in granularity_expansions
            ],
            'timeline_forecast': turnaround_prediction,
            'success_metrics': {
                'volume_growth_target': f"+{turnaround_prediction.get('potential_volume_lift', 0):,.0f} lbs",
                'customer_growth_target': "Est. +10-15% distinct customers",
                'margin_impact': "Expected margin compression offset by volume gains"
            }
        }
    
    def save_learning_state(self, filepath: str):
        """Save the current learning state for persistence"""
        
        state = {
            'zone_performance': dict(self.zone_performance),
            'zone_change_outcomes': list(self.zone_change_outcomes),
            'granularity_needs': dict(self.granularity_needs),
            'price_sensitivity_patterns': dict(self.price_sensitivity_patterns),
            'saved_at': datetime.now().isoformat()
        }
        
        with open(filepath, 'w') as f:
            json.dump(state, f, indent=2, default=str)
    
    def load_learning_state(self, filepath: str):
        """Load previously saved learning state"""
        
        if os.path.exists(filepath):
            with open(filepath, 'r') as f:
                state = json.load(f)
            
            self.zone_performance = defaultdict(lambda: {
                'volume_elasticity': 0.0,
                'customer_elasticity': 0.0,
                'optimal_zone_estimate': 5.0,
                'zone_effectiveness_curve': {},
                'sample_count': 0,
                'confidence': 0.0,
                'last_updated': datetime.now(),
                'historical_performance': []
            }, state.get('zone_performance', {}))
            
            self.zone_change_outcomes = deque(state.get('zone_change_outcomes', []), maxlen=500)
            self.granularity_needs = defaultdict(lambda: {
                'needs_expansion': False,
                'recommended_zones_between_0_1': 0,
                'zero_recommendations_count': 0,
                'evidence_score': 0.0
            }, state.get('granularity_needs', {}))
            self.price_sensitivity_patterns = defaultdict(list, state.get('price_sensitivity_patterns', {}))


def integrate_zone_optimization_with_historical_data(status_df: pd.DataFrame, raw_df: pd.DataFrame, 
                                                   historical_df: pd.DataFrame = None) -> Dict:
    """
    Main integration function for zone optimization analysis with optional historical data
    """
    
    engine = ZoneOptimizationEngine(historical_analysis_mode=True)
    
    # If historical data provided, analyze it first for immediate high-confidence recommendations
    if historical_df is not None and len(historical_df) > 0:
        print("Historical data provided - analyzing patterns...")
        historical_patterns = engine.analyze_historical_zone_changes(historical_df)
        
        # Generate immediate high-confidence recommendations based on historical patterns
        recommendations = engine.generate_immediate_high_confidence_recommendations(status_df)
        
        # Enhanced timeline prediction based on historical data
        current_trend = {
            'volume_decline_rate_pct': status_df.get('Volume_Growth_Rate', pd.Series([0])).mean(),
            'total_volume': status_df.get('Pounds_CY', pd.Series([0])).sum()
        }
        
        turnaround_prediction = engine.predict_historical_turnaround_timeline(recommendations, current_trend)
        
    else:
        print("No historical data - using current analysis only...")
        # Fall back to standard analysis
        zone_df = engine.extract_zone_features(status_df)
        zone_analysis = engine.analyze_zone_effectiveness(zone_df)
        recommendations = engine.generate_zone_recommendations(zone_analysis)
        
        current_trend = {
            'volume_decline_rate_pct': zone_df['Volume_Growth_Rate'].mean(),
            'total_volume': zone_df['Pounds_CY'].sum()
        }
        turnaround_prediction = engine.predict_volume_turnaround_timeline(recommendations, current_trend)
    
    # Executive dashboard
    executive_dashboard = engine.generate_executive_dashboard(recommendations, turnaround_prediction)
    
    return {
        'zone_analysis': getattr(engine, 'historical_patterns', {}),
        'recommendations': recommendations,
        'turnaround_prediction': turnaround_prediction,
        'executive_dashboard': executive_dashboard,
        'historical_analysis_used': historical_df is not None,
        'zone_assignments': status_df.get('Company_Combo_Key', pd.Series()).to_list() if 'Company_Combo_Key' in status_df.columns else []
    }

    def predict_historical_turnaround_timeline(self, recommendations: List[Dict], 
                                             current_trend: Dict) -> Dict:
        """
        Predict turnaround timeline based on historical patterns (accelerated for 6-week goal)
        """
        
        current_decline_rate = current_trend.get('volume_decline_rate_pct', 0)
        total_volume = current_trend.get('total_volume', 0)
        
        if total_volume == 0:
            return {'error': 'No volume data for prediction'}
        
        # Analyze recommendations by confidence and expected impact
        high_confidence_recs = [r for r in recommendations if r.get('confidence', 0) >= 0.8]
        medium_confidence_recs = [r for r in recommendations if 0.6 <= r.get('confidence', 0) < 0.8]
        
        # Calculate volume impact from historical recommendations (more aggressive than standard)
        high_conf_volume_lift = sum(self._extract_historical_volume_impact(r) for r in high_confidence_recs)
        medium_conf_volume_lift = sum(self._extract_historical_volume_impact(r) for r in medium_confidence_recs)
        
        total_potential_lift = high_conf_volume_lift + medium_conf_volume_lift
        
        # Accelerated timeline based on historical validation
        if high_conf_volume_lift > abs(current_decline_rate * total_volume * 0.8):
            # High confidence recommendations can overcome decline quickly
            turnaround_week = 3  # Very fast turnaround with historical validation
        elif total_potential_lift > abs(current_decline_rate * total_volume):
            turnaround_week = 5  # Still within 6-week target
        else:
            turnaround_week = 6  # At the limit, need more aggressive action
        
        return {
            'current_decline_rate_pct': current_decline_rate,
            'total_volume_base': total_volume,
            'potential_volume_lift': total_potential_lift,
            'high_confidence_impact': high_conf_volume_lift,
            'medium_confidence_impact': medium_conf_volume_lift,
            'predicted_turnaround_week': turnaround_week,
            'confidence': min(len(high_confidence_recs) / 5.0, 0.95),  # Higher confidence with historical data
            'acceleration_factors': [
                f"{len(high_confidence_recs)} historically-proven zone changes",
                "2+ years of historical validation data",
                "Immediate implementation possible"
            ],
            'six_week_feasibility': 'ACHIEVABLE' if turnaround_week <= 6 else 'REQUIRES_MORE_AGGRESSIVE_ACTION'
        }
    
    def _extract_historical_volume_impact(self, recommendation: Dict) -> float:
        """Extract volume impact from historical recommendations (more optimistic estimates)"""
        
        volume_at_stake = recommendation.get('total_volume_at_stake', 0)
        confidence = recommendation.get('confidence', 0.5)
        
        # Historical recommendations get more optimistic estimates due to proven patterns
        if recommendation.get('type') == 'HISTORICAL_PROVEN':
            if confidence >= 0.9:
                return volume_at_stake * 0.25  # 25% increase for very high confidence
            elif confidence >= 0.8:
                return volume_at_stake * 0.20  # 20% increase for high confidence
            elif confidence >= 0.7:
                return volume_at_stake * 0.15  # 15% increase for good confidence
        
        # Standard estimates for non-historical recommendations
        return volume_at_stake * 0.10  # 10% conservative estimate


# Function to update your existing Excel writer
def add_zone_optimization_to_excel(xw, zone_optimization_results):
    """
    Add zone optimization tabs to your existing Excel file
    Call this function from within your write_excel function
    """
    
    from openpyxl.styles import Font
    
    if not zone_optimization_results:
        return
    
    # 10 - Zone Dashboard
    dashboard = zone_optimization_results['executive_dashboard']
    
    # Executive summary
    exec_df = pd.DataFrame([dashboard['executive_summary']])
    exec_df.to_excel(xw, sheet_name="10_Zone_Dashboard", index=False, startrow=0)
    
    # Immediate action items
    if dashboard['immediate_action_items']:
        actions_df = pd.DataFrame(dashboard['immediate_action_items'])
        actions_start = exec_df.shape[0] + 3
        actions_df.to_excel(xw, sheet_name="10_Zone_Dashboard", index=False, startrow=actions_start)
        
        # Add header
        ws = xw.book["10_Zone_Dashboard"]
        ws.cell(row=actions_start, column=1, value="IMMEDIATE ACTION ITEMS - TOP PRIORITIES")
        ws.cell(row=actions_start, column=1).font = Font(bold=True)
    
    # Granularity expansion alerts (ZONE 0 flagging)
    if dashboard['granularity_expansion_alerts']:
        alerts_df = pd.DataFrame(dashboard['granularity_expansion_alerts'])
        alerts_start = actions_start + len(dashboard['immediate_action_items']) + 4
        alerts_df.to_excel(xw, sheet_name="10_Zone_Dashboard", index=False, startrow=alerts_start)
        
        # Add warning header
        ws = xw.book["10_Zone_Dashboard"]
        ws.cell(row=alerts_start, column=1, value="⚠️ ZONE 0 ALERTS - GRANULARITY EXPANSION NEEDED")
        ws.cell(row=alerts_start, column=1).font = Font(bold=True, color="FF0000")  # Red text
    
    # 11 - Detailed Zone Recommendations
    recs_df = pd.DataFrame(zone_optimization_results['recommendations'])
    if not recs_df.empty:
        recs_df.to_excel(xw, sheet_name="11_Zone_Recommendations", index=False)
        
        # Format specific columns
        try:
            from openpyxl.styles import numbers
            ws_recs = xw.book["11_Zone_Recommendations"]
            # Format numbers with commas
            for row in ws_recs.iter_rows(min_row=2, max_row=ws_recs.max_row):
                if 'volume' in str(ws_recs.cell(row=1, column=row[0].column).value).lower():
                    row[0].number_format = "#,##0"
        except:
            pass
    
    # 12 - Zone Performance Analysis
    zone_analysis = zone_optimization_results['zone_analysis']
    if zone_analysis:
        
        # Convert zone analysis to tabular format
        analysis_rows = []
        for company_combo, analysis in zone_analysis.items():
            
            base_row = {
                'Company_Combo': company_combo,
                'Company_Name': analysis['company_name'],
                'Combo_Description': analysis['combo_key'],
                'Current_Optimal_Zone': analysis['optimal_zone'],
                'Optimal_Performance_Score': analysis['optimal_performance'],
                'Price_Elasticity': analysis['price_elasticity'],
                'Total_Volume': analysis['total_volume'],
                'Zones_Tested': ', '.join(map(str, analysis['current_zones_tested'])),
                'Analysis_Type': analysis['needs_analysis']['type'],
                'Confidence': analysis['needs_analysis']['confidence'],
                'Priority': analysis['needs_analysis']['priority']
            }
            
            # Add zone performance details
            for i, zone_perf in enumerate(analysis['zone_performance_curve']):
                row = base_row.copy()
                row['Zone_Number'] = zone_perf['Zone_Suffix_Numeric']
                row['Zone_Volume_Performance'] = zone_perf['Volume_Performance']
                row['Zone_Volume_Growth_Rate'] = zone_perf['Volume_Growth_Rate']
                row['Zone_Customer_Growth_Rate'] = zone_perf['Customer_Growth_Rate']
                row['Zone_Combined_Score'] = zone_perf['Combined_Performance_Score']
                analysis_rows.append(row)
        
        if analysis_rows:
            analysis_df = pd.DataFrame(analysis_rows)
            analysis_df.to_excel(xw, sheet_name="12_Zone_Performance", index=False)
    
    # 13 - Timeline Prediction
    timeline = zone_optimization_results['turnaround_prediction']
    if timeline and 'error' not in timeline:
        
        timeline_df = pd.DataFrame([{
            'Current_Decline_Rate_Pct': timeline.get('current_decline_rate_pct', 0) * 100,
            'Total_Volume_Base': timeline['total_volume_base'],
            'Potential_Volume_Lift': timeline['potential_volume_lift'],
            'Immediate_Impact_Volume': timeline['immediate_impact_volume'],
            'Medium_Term_Impact_Volume': timeline['medium_term_impact_volume'],
            'Long_Term_Impact_Volume': timeline['long_term_impact_volume'],
            'Predicted_Turnaround_Week': timeline['predicted_turnaround_week'],
            'Prediction_Confidence': timeline['confidence'] * 100
        }])
        
        timeline_df.to_excel(xw, sheet_name="13_Timeline_Prediction", index=False)
        
        # Add dependencies and explanation
        deps_df = pd.DataFrame(timeline['key_dependencies'], columns=['Key_Dependencies'])
        deps_start = timeline_df.shape[0] + 3
        deps_df.to_excel(xw, sheet_name="13_Timeline_Prediction", index=False, startrow=deps_start)


# INTEGRATION HELPER FUNCTIONS FOR YOUR EXISTING CODE:

def create_enhanced_email_body(zone_optimization_results, source_file_name: str) -> str:
    """
    Create enhanced email body with zone optimization insights
    """
    base_body = f"Auto-generated report for {source_file_name} at {datetime.now():%Y-%m-%d %H:%M}."
    
    if not zone_optimization_results:
        return base_body
    
    zone_summary = zone_optimization_results['executive_dashboard']['executive_summary']
    enhanced_body = f"""{base_body}

ZONE OPTIMIZATION INSIGHTS:
• {zone_summary['total_zone_opportunities']} zone optimization opportunities identified
• {zone_summary['high_priority_actions']} high-priority actions recommended  
• {zone_summary['granularity_expansions_needed']} combos need granularity expansion (flagged Zone 0)
• Predicted volume turnaround: Week {zone_summary['predicted_turnaround_week']}

Key Actions Needed:
""" + "\n".join(f"• {action['company']}: {action['action']}" 
                for action in zone_optimization_results['executive_dashboard']['immediate_action_items'][:3])
    
    return enhanced_body


def add_zone_optimization_summary(summary_kpi_df: pd.DataFrame, zone_optimization_results: dict) -> pd.DataFrame:
    """
    Add zone optimization summary to existing summary KPI dataframe
    """
    if not zone_optimization_results:
        return summary_kpi_df
    
    exec_summary = zone_optimization_results['executive_dashboard']['executive_summary']
    
    # Create zone summary row
    zone_summary_row = pd.DataFrame([{
        'KPI': 'Zone Optimization Summary',
        'Value': f"{exec_summary['total_zone_opportunities']} opportunities",
        'High_Priority_Actions': exec_summary['high_priority_actions'],
        'Granularity_Expansions_Needed': exec_summary['granularity_expansions_needed'],
        'Predicted_Turnaround_Week': exec_summary['predicted_turnaround_week']
    }])
    
    return pd.concat([summary_kpi_df, zone_summary_row], ignore_index=True)


# EXAMPLE INTEGRATION CODE (commented out - copy these sections to your main script):

"""
# ADD TO YOUR IMPORTS:
from zone_optimization import (
    integrate_zone_optimization, 
    add_zone_optimization_to_excel,
    create_enhanced_email_body,
    add_zone_optimization_summary
)

# ADD AFTER status = classify_conversion(status, X=0.80, Y=0.80, Z=0.95):
try:
    zone_optimization_results = integrate_zone_optimization(status, raw)
    print(f"Zone Optimization: {zone_optimization_results['executive_dashboard']['executive_summary']['total_zone_opportunities']} opportunities identified")
except Exception as e:
    print(f"Zone optimization analysis failed: {e}")
    zone_optimization_results = None

# OPTIONALLY ADD ZONE SUMMARY TO YOUR SUMMARY KPI:
if zone_optimization_results:
    summary_kpi = add_zone_optimization_summary(summary_kpi, zone_optimization_results)

# UPDATE YOUR write_excel() FUNCTION SIGNATURE:
def write_excel(excel_path, summary_kpi, lag_sites, cust_losses, status, leads, vendor_index,
                company_yoy=None, all_weekly=None, trs_weekly=None, forecast_method="linear", 
                status_raw=None, zone_optimization_results=None):

# ADD TO YOUR write_excel() CALL:
write_excel(
    excel_path,
    summary_kpi, lag_sites, cust_losses,
    status, leads, vendor_index,
    company_yoy=company_yoy,
    all_weekly=all_weekly,
    trs_weekly=trs_weekly,
    forecast_method=forecast_method,
    status_raw=raw,
    zone_optimization_results=zone_optimization_results
)

# ADD TO THE END OF YOUR write_excel() FUNCTION (before closing the ExcelWriter):
if zone_optimization_results:
    add_zone_optimization_to_excel(xw, zone_optimization_results)

# UPDATE YOUR EMAIL BODY:
body = create_enhanced_email_body(zone_optimization_results, os.path.basename(source_path))
send_email_with_attachments(MAIL_SUBJECT, body, [excel_path, leads_csv])
"""


# COMPLETE INTEGRATION EXAMPLE FOR YOUR MAIN SCRIPT:

"""
# ADD TO YOUR EXISTING category_status_and_leads.py:

# 1. ADD IMPORTS AT TOP:
from zone_optimization import (
    integrate_zone_optimization_with_historical_data,
    add_zone_optimization_to_excel,
    create_enhanced_email_body
)

# 2. ADD AFTER YOUR status = classify_conversion(...) line:

# Load historical data
historical_files = [
    r"C:\Users\kmor6669\OneDrive - Sysco Corporation\Desktop\Pricing\historical_shrimp_1.csv",
    r"C:\Users\kmor6669\OneDrive - Sysco Corporation\Desktop\Pricing\historical_shrimp_2.csv"
]

historical_data_list = []
for file in historical_files:
    if os.path.exists(file):
        hist_df = pd.read_csv(file, dtype=str)
        hist_df = hist_df.fillna("")
        historical_data_list.append(hist_df)
        print(f"Loaded historical: {os.path.basename(file)} ({len(hist_df):,} rows)")

historical_data = pd.concat(historical_data_list, ignore_index=True) if historical_data_list else None

# Aggregate current data for zone analysis
print("Aggregating customer-week data to combo level...")
current_aggregated = raw.groupby([
    'Company Name', 'NPD Cuisine Type', 'Attribute Group ID', 
    'Price Source Type', 'Customer Account Type Code'
]).agg({
    'Pounds CY': 'sum',
    'Pounds PY': 'sum', 
    'Delta Pounds YoY': 'sum',
    'Company Customer Number': 'nunique',
    'Zone Suffix': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],
    'Fiscal Week Number': 'max'
}).reset_index()

# Run enhanced zone optimization with historical data
try:
    zone_optimization_results = integrate_zone_optimization_with_historical_data(
        status_df=current_aggregated,  # Use aggregated current data
        raw_df=raw,
        historical_df=historical_data  # Your 3M+ row historical dataset
    )
    
    if zone_optimization_results.get('historical_analysis_used'):
        print("🚀 ACCELERATED MODE: Using 2+ years of historical patterns")
        high_conf_count = len([r for r in zone_optimization_results['recommendations'] if r.get('confidence', 0) >= 0.8])
        print(f"High-confidence recommendations: {high_conf_count}")
    
    print(f"Zone Optimization: {zone_optimization_results['executive_dashboard']['executive_summary']['total_zone_opportunities']} opportunities identified")
    
except Exception as e:
    print(f"Zone optimization analysis failed: {e}")
    zone_optimization_results = None

# 3. UPDATE YOUR write_excel() FUNCTION SIGNATURE:
def write_excel(excel_path, summary_kpi, lag_sites, cust_losses, status, leads, vendor_index,
                company_yoy=None, all_weekly=None, trs_weekly=None, forecast_method="linear", 
                status_raw=None, zone_optimization_results=None):  # ADD THIS PARAMETER

# 4. UPDATE YOUR write_excel() CALL:
write_excel(
    excel_path,
    summary_kpi, lag_sites, cust_losses,
    status, leads, vendor_index,
    company_yoy=company_yoy,
    all_weekly=all_weekly,
    trs_weekly=trs_weekly,
    forecast_method=forecast_method,
    status_raw=raw,
    zone_optimization_results=zone_optimization_results  # ADD THIS LINE
)

# 5. ADD TO YOUR write_excel() FUNCTION (before closing ExcelWriter):
if zone_optimization_results:
    add_zone_optimization_to_excel(xw, zone_optimization_results)

# 6. UPDATE YOUR EMAIL:
body = create_enhanced_email_body(zone_optimization_results, os.path.basename(source_path))
send_email_with_attachments(MAIL_SUBJECT, body, [excel_path, leads_csv])
"""
